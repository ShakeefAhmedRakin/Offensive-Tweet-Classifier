{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required libraries\n",
    "\n",
    "pip install pandas scikit-learn nltk\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\shake\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up StopWords, Lemmatizer and Tokenizer\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Preprocessing Text\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    #lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    #removing @USER mentions\n",
    "    text = re.sub(r'@user', '', text)\n",
    "    \n",
    "    #removing symbols\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    preprocessed_text = text\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing Training Data\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed training data:\n",
      "                                               tweet  offensive category\n",
      "0                           ask native american take          1      UNT\n",
      "1                   go home drunk maga trump2020 url          1      TIN\n",
      "2  amazon investigating chinese employee selling ...          0     NULL\n",
      "3                 someone vetaken piece shit volcano          1      UNT\n",
      "4   obama wanted liberal amp illegals move red state          0     NULL\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('training_data.csv')\n",
    "\n",
    "train_data = train_data.drop('id', axis=1)\n",
    "\n",
    "train_data['category'].fillna('NULL', inplace=True)\n",
    "\n",
    "train_data.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "train_data['tweet'] = train_data['tweet'].apply(preprocess_text)\n",
    "train_data['tweet'] = train_data['tweet'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "train_data['offensive'] = train_data['offensive'].apply(lambda x: 1 if x == 'OFF' else 0)\n",
    "\n",
    "print(\"Preprocessed training data:\")\n",
    "print(train_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing Testing Data amd Combining with Category Data\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed testing data:\n",
      "                                               tweet  offensive category\n",
      "0  whoisq wherestheserver dumpnike declasfisa dem...          1      TIN\n",
      "1  constitutionday revered conservative hated pro...          0     NULL\n",
      "2  foxnews nra maga potus trump 2ndamendment rnc ...          0     NULL\n",
      "3  watching boomer getting news still parole alwa...          0     NULL\n",
      "4  nopasaran unity demo oppose far right london a...          1      TIN\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('testing_data.csv')  \n",
    "\n",
    "testingCat_data = pd.read_csv(\"testingCategory_data.csv\")\n",
    "\n",
    "all_ids = testingCat_data['id'].values.flatten().tolist()\n",
    "all_categories = testingCat_data['category'].values.flatten().tolist()\n",
    "\n",
    "\n",
    "id_to_category = dict(zip(all_ids, all_categories))\n",
    "\n",
    "test_data['category'] = None\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    id_value = row['id']\n",
    "    if id_value in id_to_category:\n",
    "        test_data.at[index, 'category'] = id_to_category[id_value]\n",
    "    else:\n",
    "        test_data.at[index, 'category'] = \"NULL\"\n",
    "\n",
    "test_data = test_data.drop('id', axis=1)\n",
    "\n",
    "test_data.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "test_data['tweet'] = test_data['tweet'].apply(preprocess_text)\n",
    "test_data['tweet'] = test_data['tweet'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "test_data['offensive'] = test_data['offensive'].apply(lambda x: 1 if x == 'OFF' else 0)\n",
    "\n",
    "print(\"\\nPreprocessed testing data:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data, Setting Up The TF-IDF and Setting Up The Logistic Regression Model\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['tweet'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['tweet'])\n",
    "\n",
    "\n",
    "model_offensive = LogisticRegression(max_iter=1000)\n",
    "\n",
    "\n",
    "model_category = LogisticRegression(max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing The Model On Test Data\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for offensive detection\n",
    "model_offensive.fit(X_train_tfidf, train_data['offensive'])\n",
    "y_pred_test_offensive = model_offensive.predict(X_test_tfidf)\n",
    "\n",
    "# Train for category prediction\n",
    "model_category.fit(X_train_tfidf, train_data['category'])\n",
    "y_pred_test_category = model_category.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Report\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report For Detecting Offensive Tweets On Test Data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88       620\n",
      "           1       0.87      0.36      0.51       240\n",
      "\n",
      "    accuracy                           0.81       860\n",
      "   macro avg       0.83      0.67      0.70       860\n",
      "weighted avg       0.82      0.81      0.78       860\n",
      "\n",
      "Confusion Matrix for Offensive Detection on Test Data:\n",
      "[[607  13]\n",
      " [153  87]]\n",
      "<------------------------------------------------------>\n",
      "\n",
      "Classification Report for Category Prediction on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NULL       0.80      0.97      0.88       620\n",
      "         TIN       0.69      0.35      0.47       213\n",
      "         UNT       0.50      0.04      0.07        27\n",
      "\n",
      "    accuracy                           0.79       860\n",
      "   macro avg       0.66      0.45      0.47       860\n",
      "weighted avg       0.77      0.79      0.75       860\n",
      "\n",
      "Confusion Matrix for Category Prediction on Test Data:\n",
      "[[602  17   1]\n",
      " [138  75   0]\n",
      " [  9  17   1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Classification Report For Detecting Offensive Tweets On Test Data:\\n\")\n",
    "print(classification_report(test_data['offensive'], y_pred_test_offensive))\n",
    "\n",
    "conf_matrix_test_offensive = confusion_matrix(test_data['offensive'], y_pred_test_offensive)\n",
    "print(\"Confusion Matrix for Offensive Detection on Test Data:\")\n",
    "print(conf_matrix_test_offensive)\n",
    "\n",
    "print(\"<------------------------------------------------------>\\n\")\n",
    "\n",
    "# Calculate the classification report for category prediction on test data\n",
    "classification_report_test_category = classification_report(test_data['category'], y_pred_test_category)\n",
    "print(\"Classification Report for Category Prediction on Test Data:\")\n",
    "print(classification_report_test_category)\n",
    "\n",
    "# Calculate the confusion matrix for category prediction on test data\n",
    "conf_matrix_test_category = confusion_matrix(test_data['category'], y_pred_test_category)\n",
    "print(\"Confusion Matrix for Category Prediction on Test Data:\")\n",
    "print(conf_matrix_test_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sample of Tweets\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sample tweets for prediction\n",
    "testing_tweets = [\n",
    "    \"@User Thanks so much for your help! You're amazing!\",\n",
    "    \"Just had the best time at the park with friends. Feeling grateful!\",\n",
    "    \"Huge shoutout to @User for always being there to support me. You're the best!\",\n",
    "    \"Loving this sunny weather ☀️ Ready for a productive day!\",\n",
    "    \"Spent the day reading a great book. It's so nice to unwind and relax.\",\n",
    "    \"@USER 7 fucking years.\",\n",
    "    \"@USER you're ugly.\",\n",
    "    \"@USER just kill yourself man\",\n",
    "    \"bruh yeah this shit is really ugly @USER.\",\n",
    "    \"really you ugly cunt?\",\n",
    "    \"Damn, you're a pig.\",\n",
    "    \"Why do people still fall for this shit?\",\n",
    "    \"Are you kidding with me?\",\n",
    "    \"@USER Can't believe I've waited for 3 fucking years for this.\",\n",
    "    \"Just got stuck in shit traffic again. What a day!\",\n",
    "    \"I've been dealing with this idiot all day. Can't catch a break.\",\n",
    "    \"@USER Seriously? This is the fucked up shit they come up with?\",\n",
    "    \"the hell?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Tweet Samples\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------PREDICTION ON SAMPLE TWEETS-------------------\n",
      "Tweets are that predicted to be not offensive\n",
      "--------------------------------------------------------\n",
      " >>  @User Thanks so much for your help! You're amazing!\n",
      " >>  Just had the best time at the park with friends. Feeling grateful!\n",
      " >>  Huge shoutout to @User for always being there to support me. You're the best!\n",
      " >>  Loving this sunny weather ☀️ Ready for a productive day!\n",
      " >>  Spent the day reading a great book. It's so nice to unwind and relax.\n",
      " >>  Are you kidding with me?\n",
      "--------------------------------------------------------\n",
      "Tweets are that predicted to be offensive and Targeted Insults\n",
      "--------------------------------------------------------\n",
      " >>  @USER you're ugly.\n",
      " >>  @USER just kill yourself man\n",
      " >>  bruh yeah this shit is really ugly @USER.\n",
      " >>  really you ugly cunt?\n",
      " >>  Damn, you're a pig.\n",
      " >>  Why do people still fall for this shit?\n",
      " >>  @USER Can't believe I've waited for 3 fucking years for this.\n",
      " >>  Just got stuck in shit traffic again. What a day!\n",
      " >>  I've been dealing with this idiot all day. Can't catch a break.\n",
      " >>  @USER Seriously? This is the fucked up shit they come up with?\n",
      " >>  the hell?\n",
      "--------------------------------------------------------\n",
      "Tweets are that predicted to be offensive and Untargeted Insults\n",
      "--------------------------------------------------------\n",
      " >>  @USER 7 fucking years.\n"
     ]
    }
   ],
   "source": [
    "list_of_NotOffensive = []\n",
    "list_of_OffensiveTIN = []\n",
    "list_of_OffensiveUNT = []\n",
    "\n",
    "\n",
    "for i, tweet in enumerate(testing_tweets):\n",
    "    preprocessed_tweet = preprocess_text(tweet)\n",
    "    lemmatized_tweet = tokenize_and_lemmatize(preprocessed_tweet)\n",
    "    tweet_tfidf = tfidf_vectorizer.transform([lemmatized_tweet])\n",
    "    prediction_offensive = model_offensive.predict(tweet_tfidf)\n",
    "    prediction_category = model_category.predict(tweet_tfidf)\n",
    "    \n",
    "    if (prediction_offensive[0] == 0):\n",
    "        list_of_NotOffensive.append(tweet)\n",
    "    else:\n",
    "        if (prediction_category[0] == \"UNT\"):\n",
    "            list_of_OffensiveUNT.append(tweet)\n",
    "        else:\n",
    "            list_of_OffensiveTIN.append(tweet)\n",
    "\n",
    "print(\"----------------PREDICTION ON SAMPLE TWEETS-------------------\")\n",
    "print(\"Tweets are that predicted to be not offensive\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "for i in list_of_NotOffensive:\n",
    "    print(\" >> \",i)\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Tweets are that predicted to be offensive and Targeted Insults\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "for i in list_of_OffensiveTIN:\n",
    "    print(\" >> \",i)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Tweets are that predicted to be offensive and Untargeted Insults\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "for i in list_of_OffensiveUNT:\n",
    "    print(\" >> \",i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
